{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7676386,"sourceType":"datasetVersion","datasetId":4477929},{"sourceId":7676413,"sourceType":"datasetVersion","datasetId":4477950}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-29T02:46:21.616134Z","iopub.execute_input":"2024-02-29T02:46:21.616482Z","iopub.status.idle":"2024-02-29T02:46:22.030742Z","shell.execute_reply.started":"2024-02-29T02:46:21.616448Z","shell.execute_reply":"2024-02-29T02:46:22.029704Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/bestmodel/kaggle/working/distilgpt2/config.json\n/kaggle/input/bestmodel/kaggle/working/distilgpt2/training_args.bin\n/kaggle/input/bestmodel/kaggle/working/distilgpt2/model.safetensors\n/kaggle/input/bestmodel/kaggle/working/distilgpt2/generation_config.json\n/kaggle/input/dataset/kaggle/working/test/state.json\n/kaggle/input/dataset/kaggle/working/test/dataset_info.json\n/kaggle/input/dataset/kaggle/working/test/dataset.arrow\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\n\ndef preprocess_text(text):\n    # Menghapus @mentions\n    text = re.sub(r'@\\w+', '', text)\n\n    # Menghapus URL\n    text = re.sub(r'http\\S+|www.\\S+', '', text)\n\n    # Menghapus emoticon\n    text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)\n\n    # Menghapus karakter non-ASCII\n    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n\n    # Menghapus karakter tertentu\n    text = re.sub(r'[\\^*\\\\~-].*', '', text)\n\n    # Menghapus new line\n    text = re.sub(r'\\n', ' ', text)\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-02-29T02:46:24.098153Z","iopub.execute_input":"2024-02-29T02:46:24.098599Z","iopub.status.idle":"2024-02-29T02:46:24.104817Z","shell.execute_reply.started":"2024-02-29T02:46:24.098571Z","shell.execute_reply":"2024-02-29T02:46:24.103867Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import spacy\nfrom typing import List, Dict, Any, Set, Tuple\n# Install packages as needed\n!python -m spacy download en_core_web_sm\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\ntokenizer_pattern: str = r\"\\b\\w[\\w']*\\b\"\n\nstemmer = PorterStemmer()\n\ndef tokenize_text_en(text: str, tokenizer_pattern: str) -> List[str]:\n  tokens: List[str] = re.findall(tokenizer_pattern, text)\n  return tokens\n# Lemmatize\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef lemmatize_tokens_en(tokens: List[str], nlp) -> List[str]:\n  example_lemmatized_en = []\n\n  for doc in nlp.pipe(tokens):\n    tok = [token.lemma_ for token in doc]\n    example_lemmatized_en.extend(tok)\n\n  return example_lemmatized_en\n\ndef stem_tokens_en(tokens: List[str], stemmer: PorterStemmer) -> List[str]:\n  stemmed_tokens = [stemmer.stem(token) for token in tokens]\n  return stemmed_tokens\n\nnltk_stop_words_list: List[str] = stopwords.words('english')\nnltk_stop_words_set: Set[str] = set(nltk_stop_words_list)\n\ndef remove_stop_words_en(tokens: List[str], stop_words: Dict[str, Any]) -> List[str]:\n  tokens_without_stop_words: List[str] = [\n      token\n      for token in tokens\n      if token not in stop_words\n  ]\n  return tokens_without_stop_words\n\ndef join_words_en(tokens: List[str]) -> str:\n  words: str = ' '.join(tokens)\n  return words\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T02:46:26.083034Z","iopub.execute_input":"2024-02-29T02:46:26.083394Z","iopub.status.idle":"2024-02-29T02:46:55.304320Z","shell.execute_reply.started":"2024-02-29T02:46:26.083364Z","shell.execute_reply":"2024-02-29T02:46:55.303480Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting en-core-web-sm==3.7.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_text_en(text: str,\n                                   stemmer: PorterStemmer,\n                                   tokenizer_pattern: str,\n                                   nlp) -> str:\n  tokens: List[str] = tokenize_text_en(\n    text = text,\n    tokenizer_pattern = tokenizer_pattern,\n  )\n  tokens: List[str] = lemmatize_tokens_en(\n    tokens = tokens,\n    nlp = nlp,\n  )\n  tokens: List[str] = stem_tokens_en(\n    tokens = tokens,\n    stemmer = stemmer,\n  )\n  tokens: List[str] = remove_stop_words_en(\n    tokens = tokens,\n    stop_words = nltk_stop_words_set,\n  )\n  words: str = join_words_en(\n      tokens = tokens,\n  )\n  return words","metadata":{"execution":{"iopub.status.busy":"2024-02-29T02:46:55.305814Z","iopub.execute_input":"2024-02-29T02:46:55.306293Z","iopub.status.idle":"2024-02-29T02:46:55.312513Z","shell.execute_reply.started":"2024-02-29T02:46:55.306260Z","shell.execute_reply":"2024-02-29T02:46:55.311604Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"inp = input(\"Chat: \")\nprep1 = preprocess_text(inp)\nprint(f'prep1 {prep1}')\nprep2 = preprocess_text_en(prep1, stemmer, tokenizer_pattern, nlp)\nprint(f'prep2 {prep2}')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T01:55:36.607669Z","iopub.execute_input":"2024-02-29T01:55:36.608043Z","iopub.status.idle":"2024-02-29T01:55:46.268404Z","shell.execute_reply.started":"2024-02-29T01:55:36.608016Z","shell.execute_reply":"2024-02-29T01:55:46.267372Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdin","text":"Chat:  halo my name is blabla \\n\n"},{"name":"stdout","text":"prep1 halo my name is blabla \nprep2 halo name blabla\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase\nimport torch\nfrom torch import nn\nfrom transformers import set_seed\n\ndevice: torch.device = torch.device(\"cuda\") \\\n  if torch.cuda.is_available() else torch.device(\"cpu\")\ninstruction_format: str = (\n    \"Below is an instruction that describes a task. \"\n    \"Write a response that appropriately completes the request.\\n\"\n    \"\\n\"\n    \"### Question:\\n\"\n    \"{question}\"\n    \"\\n\\n\"\n    \"### Answer:\\n\"\n    \"{answer}\"\n)\n    \ntokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained('distilgpt2')\ntokenizer.pad_token = tokenizer.eos_token\nmodel: nn.Module = AutoModelForCausalLM.from_pretrained('/kaggle/input/bestmodel/kaggle/working/distilgpt2')\nmodel.to(device)\nprint(f\"model: {model}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T02:46:58.469169Z","iopub.execute_input":"2024-02-29T02:46:58.470040Z","iopub.status.idle":"2024-02-29T02:47:10.461740Z","shell.execute_reply.started":"2024-02-29T02:46:58.470007Z","shell.execute_reply":"2024-02-29T02:47:10.460857Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7af10217f6f844859f1076db1eb98709"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22760df1ad994d7d88b0499e9cfe0174"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f1fdf719eeb481a8f1ce0e38cc148ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4d330ddaa6f44799ef8046fca9298e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33414b1f1c6148c7b0163d569de409f1"}},"metadata":{}},{"name":"stdout","text":"model: GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_answer(text):\n  split_output = text.split('### Answer:')\n  answer_split = split_output[1].split(' ')\n  answer = ''\n  index=0\n  for i in answer_split:\n      if i!='':\n          index+=1\n      if index > 2 and i=='':\n          break\n      else:\n          answer = answer+i+' '\n  answer = answer.split()\n  answer = ' '.join(answer)\n  return answer","metadata":{"execution":{"iopub.status.busy":"2024-02-29T02:47:15.878935Z","iopub.execute_input":"2024-02-29T02:47:15.879964Z","iopub.status.idle":"2024-02-29T02:47:15.885750Z","shell.execute_reply.started":"2024-02-29T02:47:15.879930Z","shell.execute_reply":"2024-02-29T02:47:15.884914Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"set_seed(87)\ndef generate_text_sampling_top_k_50(\n    input_prompt: str,\n    min_length: str = 3,\n    max_length: int = 500,\n    top_k: int = 50,\n  ) -> str:\n  input_prompt: str = instruction_format.format(\n      question=input_prompt,\n      answer='',\n  )\n  encoded_input: BatchEncoding = tokenizer(input_prompt, return_tensors='pt').to(device)\n  sampling_output_tensor: Tensor = model.generate(\n      **encoded_input,\n      min_length=min_length,\n      max_length=max_length,\n      pad_token_id=50256,\n\n      do_sample=True,\n      top_k=top_k,\n  )\n  sampling_output_text: str = tokenizer.batch_decode(sampling_output_tensor, skip_special_tokens=True)[0]\n  answer = preprocess_answer(sampling_output_text)\n  return answer\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T02:47:18.592387Z","iopub.execute_input":"2024-02-29T02:47:18.592733Z","iopub.status.idle":"2024-02-29T02:47:27.770942Z","shell.execute_reply.started":"2024-02-29T02:47:18.592705Z","shell.execute_reply":"2024-02-29T02:47:27.770121Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2024-02-29 02:47:20.260784: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-29 02:47:20.260897: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-29 02:47:20.387993: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"set_seed(87)\ndef generate_text_sampling_top_k_10(\n    input_prompt: str,\n    min_length: str = 3,\n    max_length: int = 256,\n    top_k: int = 10,\n  ) -> str:\n  input_prompt: str = instruction_format.format(\n      question=input_prompt,\n      answer='',\n  )\n  encoded_input: BatchEncoding = tokenizer(input_prompt, return_tensors='pt').to(device)\n  sampling_output_tensor: Tensor = model.generate(\n      **encoded_input,\n      min_length=min_length,\n      max_length=max_length,\n      pad_token_id=50256,\n      do_sample=True,\n      top_k=top_k,\n  )\n  sampling_output_text: str = tokenizer.batch_decode(sampling_output_tensor, skip_special_tokens=True)[0]\n  answer = preprocess_answer(sampling_output_text)\n  return answer\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T02:47:35.749691Z","iopub.execute_input":"2024-02-29T02:47:35.750060Z","iopub.status.idle":"2024-02-29T02:47:35.766642Z","shell.execute_reply.started":"2024-02-29T02:47:35.750031Z","shell.execute_reply":"2024-02-29T02:47:35.765600Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"set_seed(87)\ndef generate_text_sampling_top_p_nucleus_22(\n    input_prompt: str,\n    min_length: int = 3,\n    max_length: int =256,\n    top_p: float = 0.22,\n  ) -> str:\n  input_prompt: str = instruction_format.format(\n      question=input_prompt,\n      answer='',\n  )\n  encoded_input: BatchEncoding = tokenizer(input_prompt, return_tensors='pt').to(device)\n  sampling_output_tensor: Tensor = model.generate(\n      **encoded_input,\n      min_length=min_length,\n      max_length=max_length,\n      pad_token_id=50256,\n      do_sample=True,\n      top_p=top_p,\n      top_k=0,\n  )\n  sampling_output_text: str = tokenizer.batch_decode(sampling_output_tensor, skip_special_tokens=True)[0]\n  answer = preprocess_answer(sampling_output_text)\n  return answer\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T02:47:37.446467Z","iopub.execute_input":"2024-02-29T02:47:37.447138Z","iopub.status.idle":"2024-02-29T02:47:37.454197Z","shell.execute_reply.started":"2024-02-29T02:47:37.447105Z","shell.execute_reply":"2024-02-29T02:47:37.453074Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"set_seed(87)\ndef generate_text_sampling_top_p_nucleus_92(\n    input_prompt: str,\n    min_length: int = 3,\n    max_length: int =256,\n    top_p: float = 0.92,\n  ) -> str:\n  input_prompt: str = instruction_format.format(\n      question=input_prompt,\n      answer='',\n  )\n  encoded_input: BatchEncoding = tokenizer(input_prompt, return_tensors='pt').to(device)\n  sampling_output_tensor: Tensor = model.generate(\n      **encoded_input,\n      min_length=min_length,\n      max_length=max_length,\n      do_sample=True,\n      pad_token_id=50256,\n      top_p=top_p,\n      top_k=0,\n  )\n  sampling_output_text: str = tokenizer.batch_decode(sampling_output_tensor, skip_special_tokens=True)[0]\n  answer = preprocess_answer(sampling_output_text)\n  return answer\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T02:47:38.705957Z","iopub.execute_input":"2024-02-29T02:47:38.706752Z","iopub.status.idle":"2024-02-29T02:47:38.713735Z","shell.execute_reply.started":"2024-02-29T02:47:38.706719Z","shell.execute_reply":"2024-02-29T02:47:38.712662Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def preprocess_question(text):\n    prep1 = preprocess_text(text)\n    return preprocess_text_en(prep1, stemmer, tokenizer_pattern, nlp)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T02:47:39.837816Z","iopub.execute_input":"2024-02-29T02:47:39.838390Z","iopub.status.idle":"2024-02-29T02:47:39.842775Z","shell.execute_reply.started":"2024-02-29T02:47:39.838364Z","shell.execute_reply":"2024-02-29T02:47:39.841697Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"lol = 'i already dm my email'\nquest = preprocess_question(lol)\nprint(f'prep2 {quest}')\nprint(f'answer: {generate_text_sampling_top_p_nucleus_22(quest)}')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T02:48:34.781763Z","iopub.execute_input":"2024-02-29T02:48:34.782125Z","iopub.status.idle":"2024-02-29T02:48:36.175379Z","shell.execute_reply.started":"2024-02-29T02:48:34.782097Z","shell.execute_reply":"2024-02-29T02:48:36.174515Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"prep2 I alreadi dm email\nanswer: We've responded to your DM. Thank you.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}